# Use Case: Sequential Chains
# - Processing data through multiple steps in a fixed order
# - Creating multi-stage reasoning or transformation pipelines
# - Building question-and-answer systems with intermediate steps
# - Automating workflows that require step-by-step processing

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SequentialChain

# Load environment variables
load_dotenv()

# Check for API key
if os.getenv("OPENAI_API_KEY") is None:
    print("Error: OPENAI_API_KEY environment variable not set.")
    exit()

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
print("LLM Initialized successfully!\n")

def demonstrate_sequential_chain():
    # Chain 1: Generate a question about a topic
    question_template = "Generate a simple technical question about {topic}."
    question_prompt = PromptTemplate(input_variables=["topic"], template=question_template)
    question_chain = LLMChain(llm=llm, prompt=question_prompt, output_key="question")

    # Chain 2: Answer the question generated by Chain 1
    answer_template = "Answer the following question concisely: {question}"
    answer_prompt = PromptTemplate(input_variables=["question"], template=answer_template)
    answer_chain = LLMChain(llm=llm, prompt=answer_prompt, output_key="answer")

    # Combine chains sequentially
    sequential_chain = SequentialChain(
        chains=[question_chain, answer_chain],
        input_variables=["topic"],  # Input for the first chain
        output_variables=["question", "answer"],  # Outputs we want to see
        verbose=True
    )

    # Run the sequential chain
    topic = "Zoo in Bangalore"
    print(f"\n--- Sequential Chain Demo ---")
    print(f"Topic: {topic}\n")
    
    result = sequential_chain.invoke({"topic": topic})
    
    print(f"\nResults:\n")
    print(f"Generated Question: {result['question']}")
    print(f"Generated Answer: {result['answer']}\n")

def interactive_sequential_chat():
    # Chain 1: Analyze the user's question
    analyze_template = "Analyze this question: {user_input}. Guess why he is asking about it ? Answer in one short sentence."
    analyze_prompt = PromptTemplate(input_variables=["user_input"], template=analyze_template)
    analyze_chain = LLMChain(llm=llm, prompt=analyze_prompt, output_key="analysis")
    
    # Chain 2: Generate a detailed response based on the analysis
    response_template = "The user asked: {user_input}\nYour analysis: {analysis}\nProvide a helpful, informative response."
    response_prompt = PromptTemplate(input_variables=["user_input", "analysis"], template=response_template)
    response_chain = LLMChain(llm=llm, prompt=response_prompt, output_key="response")
    
    # Create the sequential chain
    chat_chain = SequentialChain(
        chains=[analyze_chain, response_chain],
        input_variables=["user_input"],
        output_variables=["analysis", "response"],
        verbose=False
    )
    
    print("\n--- Interactive Sequential Chain Chat ---")
    print("Type 'exit' or 'quit' to end the chat.\n")
    print("This chat first analyzes your question, then provides a response based on that analysis.\n")
    
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        if not user_input.strip():
            print("AI: Please enter some text.")
            continue
        
        # Run the sequential chain with user input
        result = chat_chain.invoke({"user_input": user_input})
        
        print(f"(Analysis: {result['analysis']})\n")
        print("\n\n\n\n\n\n\n")
        print(f"AI: {result['response']}\n")
        



if __name__ == "__main__":
    demonstrate_sequential_chain()
    interactive_sequential_chat()
    print("Chat session ended. Thank you for using the sequential chains demo!")



# Travel Planning Assistant - Practical Examples

# Example 1: Multi-step travel destination finder
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SequentialChain

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# Chain 1: Generate destination options based on preferences
destinations_template = "Based on the traveler's preferences (climate: {climate}, activities: {activities}, budget: {budget}), suggest 3 suitable travel destinations. Only list the destination names separated by commas."
destinations_prompt = PromptTemplate(input_variables=["climate", "activities", "budget"], template=destinations_template)
destinations_chain = LLMChain(llm=llm, prompt=destinations_prompt, output_key="destinations")

# Chain 2: Select the best destination with reasoning
selection_template = "From these destinations: {destinations}, select the single BEST option for a traveler with these preferences - climate: {climate}, activities: {activities}, budget: {budget}. Explain your choice with pros and cons."
selection_prompt = PromptTemplate(input_variables=["destinations", "climate", "activities", "budget"], template=selection_template)
selection_chain = LLMChain(llm=llm, prompt=selection_prompt, output_key="recommendation")

# Connect the chains
destination_recommender = SequentialChain(
    chains=[destinations_chain, selection_chain],
    input_variables=["climate", "activities", "budget"],
    output_variables=["destinations", "recommendation"],
    verbose=True
)

# Use the sequential chain
result = destination_recommender.invoke({
    "climate": "warm and sunny", 
    "activities": "hiking, swimming, and cultural experiences", 
    "budget": "mid-range"
})

print(f"Initial Destinations: {result['destinations']}")
print(f"Final Recommendation: {result['recommendation']}")
"""


"""
Welcome to the LLM Zoo, where AI models roam wild and free!, What animal represents the Sequential chains in this zoo
"""


# Summary: This file demonstrates how to use LangChain's SequentialChain to create a pipeline of
# operations where the output of one step becomes the input to the next. It shows how to construct
# a simple question-answer system with an intermediate step, and then applies this concept to create
# an interactive chat that analyzes user input before generating a response.
