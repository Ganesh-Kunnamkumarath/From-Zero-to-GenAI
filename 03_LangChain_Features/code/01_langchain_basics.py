import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SequentialChain
from langchain.memory import ConversationBufferMemory
from langchain.output_parsers import CommaSeparatedListOutputParser, PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List

# --- Setup ---
# Load environment variables (especially OPENAI_API_KEY)
load_dotenv()

# Ensure your OPENAI_API_KEY is set in your environment or a .env file
if os.getenv("OPENAI_API_KEY") is None:
    print("Error: OPENAI_API_KEY environment variable not set.")
    exit()

# Initialize the LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
print("LLM Initialized.")
print("----------------------------------------\n")


# --- 1. Basic LLM Call ---
print("--- 1. Basic LLM Call ---")
basic_prompt = "What is LangChain in one sentence?"
response = llm.invoke(basic_prompt)
print(f"Prompt: {basic_prompt}")
print(f"Response: {response.content}")
print("----------------------------------------\n")



# --- 2. Prompt Templates & LLMChain ---
print("--- 2. Prompt Templates & LLMChain ---")
# Template for defining a term with a specified number of lines
define_template = "Do a {lines} liner SWOT analysis for the term: {term}"
define_prompt = PromptTemplate(input_variables=["term", "lines"], template=define_template)

# Create a simple chain
define_chain = LLMChain(llm=llm, prompt=define_prompt)

# Run the chain
term_to_define = "Large Language Model"
lines_to_generate = 1
definition = define_chain.run({"term": term_to_define, "lines": lines_to_generate})
print(f"Defining '{term_to_define}' in {lines_to_generate} lines:")
print(definition)
print("----------------------------------------\n")



# --- 3. Sequential Chains ---
print("--- 3. Sequential Chains ---")
# Chain 1: Generate a question about a topic
question_template = "Generate a simple technical question about {topic}."
question_prompt = PromptTemplate(input_variables=["topic"], template=question_template)
question_chain = LLMChain(llm=llm, prompt=question_prompt, output_key="question")

# Chain 2: Answer the question generated by Chain 1
answer_template = "Answer the following question concisely: {question}"
answer_prompt = PromptTemplate(input_variables=["question"], template=answer_template)
answer_chain = LLMChain(llm=llm, prompt=answer_prompt, output_key="answer")

# Combine chains sequentially
sequential_chain = SequentialChain(
    chains=[question_chain, answer_chain],
    input_variables=["topic"], # Input for the first chain
    output_variables=["question", "answer"], # Desired outputs from the sequence
    verbose=True
)

# Run the sequential chain
topic = "Artificial Intelligence"
result = sequential_chain({"topic": topic})
print(f"\nTopic: {topic}")
print(f"Generated Question: {result['question']}")
print(f"Generated Answer: {result['answer']}")
print("----------------------------------------\n")



# --- 4. Conversation Memory ---
print("--- 4. Conversation Memory ---")
# Use the 'define_chain' from example 2, but add memory
memory = ConversationBufferMemory(memory_key="chat_history", input_key="term") # input_key matches LLMChain input

conversation_chain = LLMChain(
    llm=llm,
    prompt=define_prompt, # Reusing the prompt from example 2
    memory=memory,
    verbose=True
)

# First interaction
print("Interaction 1:")
response1 = conversation_chain.run({"term": "Generative AI", "lines": 3})
print(f"Definition of Generative AI: {response1}\n")

# Second interaction (memory is used)
print("Interaction 2:")
response2 = conversation_chain.run({"term": "Prompt Engineering", "lines": 2})
print(f"Definition of Prompt Engineering: {response2}\n")

# View memory
print("Current Memory:")
print(memory.load_memory_variables({}))
print("----------------------------------------\n")


# --- 5. Output Parsers ---
print("--- 5. Output Parsers ---")

# Example 5a: Comma Separated List Output Parser
print("\n--- 5a. Comma Separated List Output Parser ---")
list_parser = CommaSeparatedListOutputParser()
list_format_instructions = list_parser.get_format_instructions()

list_prompt_template = "List 5 key components of the LangChain framework.\n{format_instructions}"
list_prompt = PromptTemplate(
    template=list_prompt_template,
    input_variables=[],
    partial_variables={"format_instructions": list_format_instructions}
)

list_chain = LLMChain(llm=llm, prompt=list_prompt)
output_list_str = list_chain.run({}) # No input variables needed
parsed_list = list_parser.parse(output_list_str)

print(f"Raw Output:\n{output_list_str}")
print(f"Parsed List:\n{parsed_list}")

# Example 5b: Pydantic Output Parser (Structured Output)
print("\n--- 5b. Pydantic Output Parser ---")

# Define the desired data structure
class ConceptDefinition(BaseModel):
    concept_name: str = Field(description="The name of the concept")
    definition: str = Field(description="A concise definition of the concept")
    related_terms: List[str] = Field(description="A list of related terms")

# Set up the Pydantic parser
pydantic_parser = PydanticOutputParser(pydantic_object=ConceptDefinition)
pydantic_format_instructions = pydantic_parser.get_format_instructions()

# Create the prompt
pydantic_prompt_template = "Define the concept '{concept}'.\n{format_instructions}"
pydantic_prompt = PromptTemplate(
    template=pydantic_prompt_template,
    input_variables=["concept"],
    partial_variables={"format_instructions": pydantic_format_instructions}
)

pydantic_chain = LLMChain(llm=llm, prompt=pydantic_prompt)
concept_to_define = "Vector Database"
output_structured_str = pydantic_chain.run(concept_to_define)

try:
    parsed_structured = pydantic_parser.parse(output_structured_str)
    print(f"Raw Output:\n{output_structured_str}")
    print(f"Parsed Structured Output:\n{parsed_structured}")
except Exception as e:
    print(f"Error parsing Pydantic output: {e}")
    print(f"Raw Output was:\n{output_structured_str}")

print("----------------------------------------\n")

print("Basic LangChain examples finished.")
